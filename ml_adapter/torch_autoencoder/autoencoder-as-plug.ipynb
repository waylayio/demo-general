{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46523964-f381-46ff-b009-76b32cdc3b00",
   "metadata": {},
   "source": [
    "# A pytorch ML adapter demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd854b-9a6e-4bac-9650-5d86b3ea56d5",
   "metadata": {},
   "source": [
    "#### Preamble\n",
    "Demo of a relatively small pytorch model.\n",
    "This notebook uses the [ml_adapter_torch](../../env/ml_adapter_torch) dependencies.\n",
    "To start: \n",
    "```\n",
    "bin/jupyter_notebook env/ml_adapter_torch ml_adapter/torch_autoencoder\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a53829b8-6fde-490f-a870-c5f540f3e03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d623cc-fad6-4d2c-9e43-8bb9ccde3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sdk profile used to connect\n",
    "PROFILE='_default_'\n",
    "LOG_LEVEL='INFO'\n",
    "MODEL_NAME='autoencoderV1'\n",
    "MODEL_VERSION='1.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dfbefcf-84f8-40d2-9393-ed3988630147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup INFO logging to see http requests made.\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=LOG_LEVEL,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e989be6-88be-40c9-b80a-e77d5f152eb8",
   "metadata": {},
   "source": [
    "## Pytorch example\n",
    "\n",
    "A simple auto-encoder pytorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fbe4087-26be-4945-8f1a-96fab99d3669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b779cd6-2c39-4efa-95ef-6a8389ec7820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "\n",
       "<span class=\"k\">class</span> <span class=\"nc\">AutoEncoder</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n",
       "    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
       "        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n",
       "<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;</span>\n",
       "<span class=\"sd\">        Create a simple AutoEncoder</span>\n",
       "<span class=\"sd\">        &quot;&quot;&quot;</span>\n",
       "        <span class=\"c1\"># Use an AutoEncoder and try to reconstruct both signals (the 20 samples back)</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">encoder</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">),</span>\n",
       "        <span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">decoder</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">),</span>\n",
       "        <span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n",
       "        <span class=\"n\">encoded</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">encoder</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">decoded</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">decoder</span><span class=\"p\">(</span><span class=\"n\">encoded</span><span class=\"p\">)</span>\n",
       "        <span class=\"k\">return</span> <span class=\"n\">decoded</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\n",
       "\\PY{k}{class} \\PY{n+nc}{AutoEncoder}\\PY{p}{(}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Module}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{k}{def} \\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n+nb}{super}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{+w}{        }\\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "\\PY{l+s+sd}{        Create a simple AutoEncoder}\n",
       "\\PY{l+s+sd}{        \\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "        \\PY{c+c1}{\\PYZsh{} Use an AutoEncoder and try to reconstruct both signals (the 20 samples back)}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{encoder} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Sequential}\\PY{p}{(}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{20}\\PY{p}{,} \\PY{l+m+mi}{10}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{ReLU}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{10}\\PY{p}{,} \\PY{l+m+mi}{5}\\PY{p}{)}\\PY{p}{,}\n",
       "        \\PY{p}{)}\n",
       "\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{decoder} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Sequential}\\PY{p}{(}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{5}\\PY{p}{,} \\PY{l+m+mi}{10}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{ReLU}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{10}\\PY{p}{,} \\PY{l+m+mi}{20}\\PY{p}{)}\\PY{p}{,}\n",
       "        \\PY{p}{)}\n",
       "\n",
       "    \\PY{k}{def} \\PY{n+nf}{forward}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{x}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n}{encoded} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{encoder}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n",
       "        \\PY{n}{decoded} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{decoder}\\PY{p}{(}\\PY{n}{encoded}\\PY{p}{)}\n",
       "        \\PY{k}{return} \\PY{n}{decoded}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "import torch\n",
       "\n",
       "class AutoEncoder(torch.nn.Module):\n",
       "    def __init__(self):\n",
       "        super().__init__()\n",
       "        \"\"\"\n",
       "        Create a simple AutoEncoder\n",
       "        \"\"\"\n",
       "        # Use an AutoEncoder and try to reconstruct both signals (the 20 samples back)\n",
       "        self.encoder = torch.nn.Sequential(\n",
       "            torch.nn.Linear(20, 10),\n",
       "            torch.nn.ReLU(),\n",
       "            torch.nn.Linear(10, 5),\n",
       "        )\n",
       "\n",
       "        self.decoder = torch.nn.Sequential(\n",
       "            torch.nn.Linear(5, 10),\n",
       "            torch.nn.ReLU(),\n",
       "            torch.nn.Linear(10, 20),\n",
       "        )\n",
       "\n",
       "    def forward(self, x):\n",
       "        encoded = self.encoder(x)\n",
       "        decoded = self.decoder(encoded)\n",
       "        return decoded"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we saved our model class in a `autoencoder.py` file\n",
    "from IPython.display import Code, Markdown\n",
    "display(Code(filename='autoencoder.py'))\n",
    "from autoencoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5409ee2-ffb0-450b-91a6-9be5b857c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some pretrained weights\n",
    "weights_path = 'AutoEncoderWeights.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22a7a6e6-ece7-49d3-8bba-a128ebc621c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=5, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = AutoEncoder()\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f74982-aea0-4d08-8ef7-71c7fec68a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1321,  1.0263,  1.2909,  0.8727, -1.0449,  0.7338, -2.2298,  0.5470,\n",
       "        -0.6775, -0.8363,  1.4936,  0.4840, -1.4861,  0.9581,  1.0316,  1.5117,\n",
       "        -1.1868,  1.2446,  0.0825,  0.1862])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = torch.randn(20, dtype=torch.float32)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cef5213-7a07-4360-80b1-6bd352a188a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0756,  0.1232,  0.0354,  0.0153,  0.1208,  0.0407,  0.0919,  0.0967,\n",
       "         0.0121,  0.2355,  0.1027,  0.0877,  0.1641,  0.0342, -0.0093,  0.0025,\n",
       "         0.0854, -0.1048,  0.0171,  0.1375], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(x_data)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc56c20c-e56c-476b-ba70-27e40a479b1f",
   "metadata": {},
   "source": [
    "## The adapter\n",
    "The `V1TorchAdapter` from the `ml_adapter.torch` module wraps our model in a script that can be used as a waylay webscript or plug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7023e2eb-4ebb-4f5e-a436-5e11b07222e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a local archive location\n",
    "ARCHIVE_LOC = 'autoencoder-pytorch'\n",
    "# make sure its empty\n",
    "!rm -fr autoencoder-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc2c179-9eb8-4406-ba6a-ef38f6786336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_adapter.torch import V1TorchAdapter\n",
    "# create an ML adapter to wrap our model\n",
    "# by using a `weights.pt` postfix we are storing only the weights when serializing the model\n",
    "MODEL_PATH='model-weights.pt'\n",
    "adapter = V1TorchAdapter(model=model, model_path='model-weights.pt', location=ARCHIVE_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47bc96ed-9f3c-4958-a718-4eb7b18735d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.07556847482919693,\n",
       "   0.12324532866477966,\n",
       "   0.03544365242123604,\n",
       "   0.015328444540500641,\n",
       "   0.1207667887210846,\n",
       "   0.04073508456349373,\n",
       "   0.09185446798801422,\n",
       "   0.09667737036943436,\n",
       "   0.01210200134664774,\n",
       "   0.2355400025844574,\n",
       "   0.10270106792449951,\n",
       "   0.08770085126161575,\n",
       "   0.16406965255737305,\n",
       "   0.03420832008123398,\n",
       "   -0.00929244700819254,\n",
       "   0.002525750547647476,\n",
       "   0.08540024608373642,\n",
       "   -0.10478955507278442,\n",
       "   0.017097875475883484,\n",
       "   0.13751526176929474]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the adapter exposes your model with a REST-compatible interface\n",
    "result = await adapter.call({\"instances\": [ x_data.tolist() ]}) \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7c77d97-61a2-488c-8e07-8c894b7a8f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autoencoder.py <ml_adapter.base.assets.python.PythonScriptAsset>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because we store only weights, the adapter archive needs to now about autoencode model class:\n",
    "await adapter.add_script('autoencoder.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9872aa-6cab-43ea-9fd4-87f1a43390fc",
   "metadata": {},
   "source": [
    "### Creating the plug\n",
    "Tell the adapter to configure itself as a plug: this generates a number of _assets_ that will be uploaded and define the plug behaviour\n",
    "* a `plug.json` _manifest_ file that defines the name, version, _runtime_, deploy settings, metadata ...\n",
    "* a `requirements.txt` package dependencies file\n",
    "* a `main.py` webscript script\n",
    "* additional scripts we added above, like the `autoencoder.py` and the `model-weights.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f68556cd-921d-495d-93d1-a94c7897d1e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## configure any 'memory' or 'cpu' deploy settings \n",
    "deploy_overrides = {'limits' : { 'memory': '2G' }, 'requests' : { 'memory' : '1G' }}\n",
    "## configure the webscript to use our model\n",
    "adapter = adapter.as_plug({\n",
    "    'name': MODEL_NAME, \n",
    "    'description':'pytorch autoencoder for caats', \n",
    "    'deploy' : deploy_overrides                                                                                          \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63199dc8-edb2-46f9-8ef3-4f398fa59d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del adapter.assets.children[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e75891d-87fc-4b58-925f-e779595a3b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openapi.json',\n",
       " 'requirements.txt',\n",
       " 'main.py',\n",
       " 'model-weights.pt',\n",
       " 'autoencoder.py',\n",
       " 'plug.json']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await adapter.save()\n",
    "list(a.path for a in adapter.assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c2ce077-f293-4c2f-9c6f-7784ddb3fdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># ml_adapter.torch.adapter.V1TorchAdapter model adapter</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">os</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">ml_adapter.api.data</span> <span class=\"kn\">import</span> <span class=\"n\">v1</span> <span class=\"k\">as</span> <span class=\"n\">V1</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">ml_adapter.torch.adapter</span> <span class=\"kn\">import</span> <span class=\"n\">V1TorchAdapter</span>\n",
       "\n",
       "<span class=\"c1\"># optional type alias for plug response</span>\n",
       "<span class=\"n\">StatusAndRawData</span> <span class=\"o\">=</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">V1</span><span class=\"o\">.</span><span class=\"n\">V1PredictionResponse</span><span class=\"o\">|</span><span class=\"n\">V1</span><span class=\"o\">.</span><span class=\"n\">V1ErrorResponse</span><span class=\"p\">]</span>\n",
       "\n",
       "<span class=\"n\">STATE_OK</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;PREDICTED&#39;</span>\n",
       "<span class=\"n\">STATE_NOK</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;FAILED&#39;</span>\n",
       "\n",
       "<span class=\"n\">MODEL_PATH</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">&#39;MODEL_PATH&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;model-weights.pt&#39;</span><span class=\"p\">)</span>\n",
       "<span class=\"n\">MODEL_CLASS</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">&#39;MODEL_CLASS&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;autoencoder.AutoEncoder&#39;</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Initialize the model adapter.</span>\n",
       "<span class=\"c1\"># Provide a `model` argument if you want to create/load the model yourself.</span>\n",
       "<span class=\"n\">adapter</span> <span class=\"o\">=</span> <span class=\"n\">V1TorchAdapter</span><span class=\"p\">(</span>\n",
       "    <span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"n\">MODEL_PATH</span><span class=\"p\">,</span> <span class=\"n\">model_class</span><span class=\"o\">=</span><span class=\"n\">MODEL_CLASS</span>\n",
       "<span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">properties</span><span class=\"p\">:</span> <span class=\"n\">V1</span><span class=\"o\">.</span><span class=\"n\">V1Request</span><span class=\"p\">,</span> <span class=\"n\">console</span><span class=\"p\">,</span> <span class=\"n\">logger</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">StatusAndRawData</span><span class=\"p\">:</span>\n",
       "    <span class=\"k\">try</span><span class=\"p\">:</span>\n",
       "        <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">adapter</span><span class=\"o\">.</span><span class=\"n\">call</span><span class=\"p\">(</span><span class=\"n\">properties</span><span class=\"p\">)</span>\n",
       "        <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">STATE_OK</span><span class=\"p\">,</span> <span class=\"n\">result</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">except</span> <span class=\"ne\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">err</span><span class=\"p\">:</span>\n",
       "        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">exception</span><span class=\"p\">(</span><span class=\"n\">err</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">error_message</span> <span class=\"o\">=</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">err</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">console</span><span class=\"o\">.</span><span class=\"n\">error</span><span class=\"p\">(</span><span class=\"n\">error_message</span><span class=\"p\">)</span>\n",
       "        <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">STATE_NOK</span><span class=\"p\">,</span> <span class=\"p\">{</span> <span class=\"s1\">&#39;error&#39;</span><span class=\"p\">:</span> <span class=\"n\">error_message</span><span class=\"p\">,</span> <span class=\"s1\">&#39;predictions&#39;</span><span class=\"p\">:</span> <span class=\"p\">[]</span> <span class=\"p\">})</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{} ml\\PYZus{}adapter.torch.adapter.V1TorchAdapter model adapter}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{os}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{ml\\PYZus{}adapter}\\PY{n+nn}{.}\\PY{n+nn}{api}\\PY{n+nn}{.}\\PY{n+nn}{data} \\PY{k+kn}{import} \\PY{n}{v1} \\PY{k}{as} \\PY{n}{V1}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{ml\\PYZus{}adapter}\\PY{n+nn}{.}\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{adapter} \\PY{k+kn}{import} \\PY{n}{V1TorchAdapter}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} optional type alias for plug response}\n",
       "\\PY{n}{StatusAndRawData} \\PY{o}{=} \\PY{n+nb}{tuple}\\PY{p}{[}\\PY{n+nb}{str}\\PY{p}{,} \\PY{n}{V1}\\PY{o}{.}\\PY{n}{V1PredictionResponse}\\PY{o}{|}\\PY{n}{V1}\\PY{o}{.}\\PY{n}{V1ErrorResponse}\\PY{p}{]}\n",
       "\n",
       "\\PY{n}{STATE\\PYZus{}OK} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{PREDICTED}\\PY{l+s+s1}{\\PYZsq{}}\n",
       "\\PY{n}{STATE\\PYZus{}NOK} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{FAILED}\\PY{l+s+s1}{\\PYZsq{}}\n",
       "\n",
       "\\PY{n}{MODEL\\PYZus{}PATH} \\PY{o}{=} \\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{o}{.}\\PY{n}{get}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{MODEL\\PYZus{}PATH}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{model\\PYZhy{}weights.pt}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "\\PY{n}{MODEL\\PYZus{}CLASS} \\PY{o}{=} \\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{o}{.}\\PY{n}{get}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{MODEL\\PYZus{}CLASS}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{autoencoder.AutoEncoder}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Initialize the model adapter.}\n",
       "\\PY{c+c1}{\\PYZsh{} Provide a `model` argument if you want to create/load the model yourself.}\n",
       "\\PY{n}{adapter} \\PY{o}{=} \\PY{n}{V1TorchAdapter}\\PY{p}{(}\n",
       "    \\PY{n}{model\\PYZus{}path}\\PY{o}{=}\\PY{n}{MODEL\\PYZus{}PATH}\\PY{p}{,} \\PY{n}{model\\PYZus{}class}\\PY{o}{=}\\PY{n}{MODEL\\PYZus{}CLASS}\n",
       "\\PY{p}{)}\n",
       "\n",
       "\\PY{k}{async} \\PY{k}{def} \\PY{n+nf}{execute}\\PY{p}{(}\\PY{n}{properties}\\PY{p}{:} \\PY{n}{V1}\\PY{o}{.}\\PY{n}{V1Request}\\PY{p}{,} \\PY{n}{console}\\PY{p}{,} \\PY{n}{logger}\\PY{p}{)} \\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZgt{}} \\PY{n}{StatusAndRawData}\\PY{p}{:}\n",
       "    \\PY{k}{try}\\PY{p}{:}\n",
       "        \\PY{n}{result} \\PY{o}{=} \\PY{k}{await} \\PY{n}{adapter}\\PY{o}{.}\\PY{n}{call}\\PY{p}{(}\\PY{n}{properties}\\PY{p}{)}\n",
       "        \\PY{k}{return} \\PY{p}{(}\\PY{n}{STATE\\PYZus{}OK}\\PY{p}{,} \\PY{n}{result}\\PY{p}{)}\n",
       "    \\PY{k}{except} \\PY{n+ne}{Exception} \\PY{k}{as} \\PY{n}{err}\\PY{p}{:}\n",
       "        \\PY{n}{logger}\\PY{o}{.}\\PY{n}{exception}\\PY{p}{(}\\PY{n}{err}\\PY{p}{)}\n",
       "        \\PY{n}{error\\PYZus{}message} \\PY{o}{=} \\PY{n+nb}{str}\\PY{p}{(}\\PY{n}{err}\\PY{p}{)}\n",
       "        \\PY{n}{console}\\PY{o}{.}\\PY{n}{error}\\PY{p}{(}\\PY{n}{error\\PYZus{}message}\\PY{p}{)}\n",
       "        \\PY{k}{return} \\PY{p}{(}\\PY{n}{STATE\\PYZus{}NOK}\\PY{p}{,} \\PY{p}{\\PYZob{}} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{error}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{n}{error\\PYZus{}message}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{predictions}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{p}{[}\\PY{p}{]} \\PY{p}{\\PYZcb{}}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "\n",
       "# ml_adapter.torch.adapter.V1TorchAdapter model adapter\n",
       "import os\n",
       "from ml_adapter.api.data import v1 as V1\n",
       "from ml_adapter.torch.adapter import V1TorchAdapter\n",
       "\n",
       "# optional type alias for plug response\n",
       "StatusAndRawData = tuple[str, V1.V1PredictionResponse|V1.V1ErrorResponse]\n",
       "\n",
       "STATE_OK = 'PREDICTED'\n",
       "STATE_NOK = 'FAILED'\n",
       "\n",
       "MODEL_PATH = os.environ.get('MODEL_PATH', 'model-weights.pt')\n",
       "MODEL_CLASS = os.environ.get('MODEL_CLASS', 'autoencoder.AutoEncoder')\n",
       "\n",
       "# Initialize the model adapter.\n",
       "# Provide a `model` argument if you want to create/load the model yourself.\n",
       "adapter = V1TorchAdapter(\n",
       "    model_path=MODEL_PATH, model_class=MODEL_CLASS\n",
       ")\n",
       "\n",
       "async def execute(properties: V1.V1Request, console, logger) -> StatusAndRawData:\n",
       "    try:\n",
       "        result = await adapter.call(properties)\n",
       "        return (STATE_OK, result)\n",
       "    except Exception as err:\n",
       "        logger.exception(err)\n",
       "        error_message = str(err)\n",
       "        console.error(error_message)\n",
       "        return (STATE_NOK, { 'error': error_message, 'predictions': [] })"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets have a look at the generated python plug:\n",
    "display(Code(filename=f'{ARCHIVE_LOC}/main.py'))\n",
    "\n",
    "# You could adapt this script to have specific error handling or handling of request/response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d65e204-c421-4026-a337-330dcbeca22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once stored, the adapter can be restored later with\n",
    "# adapter = await V1TorchAdapter(model_path='model-weights.pt', model_class=AutoEncoder, location=ARCHIVE_LOC).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef9453f4-3352-4eeb-a79c-4b3656a70624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openapi.json',\n",
       " 'requirements.txt',\n",
       " 'main.py',\n",
       " 'model-weights.pt',\n",
       " 'autoencoder.py',\n",
       " 'plug.json']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a.path for a in adapter.assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e0183-cf8c-4225-965e-89b75b3c781d",
   "metadata": {},
   "source": [
    "### Uploading the plug using the SDK\n",
    "To upload these assets and create a plug, we need to call the [create plug](https://docs.waylay.io/openapi/public/redocly/registry.html#tag/Plugs/operation/create_plugs) REST api. \n",
    "\n",
    "The code belows uses the `ml_tool` plugin to handle this.\n",
    "Alternatively you could call `await adapter.save_archive()`\n",
    "which creates an `autoencoder-pytorch.tar.gz` archive that you can upload as a plug with `curl` or using the waylay console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbb7b49c-cc60-4e8a-afae-a2908a6caf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from waylay.sdk import WaylayClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecca79ac-ad09-43c4-876a-7a5e9fd7b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the SDK client\n",
    "client = WaylayClient.from_profile('staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70434f0b-6bdc-4ca8-8214-0dd40ba5bfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 14:00:15 INFO     HTTP Request: POST https://api-aws-dev.waylay.io/accounts/v1/tokens?grant_type=client_credentials \"HTTP/1.1 200 OK\"\n",
      "2024-06-12 14:00:15 INFO     HTTP Request: POST https://api-aws-dev.waylay.io/registry/v2/plugs/?draft=false&comment=&async=true \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Building and deploying plug autoencoderV1@0.0.1',\n",
       " '_links': {'event': {'href': 'https://api-aws-dev.waylay.io/registry/v2/jobs/events?type=verify&id=740799ef-d515-4704-8718-903851c9899e$zrLKLX-4edcrChChVBWrB&children=true'},\n",
       "  'job': {'href': 'https://api-aws-dev.waylay.io/registry/v2/jobs/verify/740799ef-d515-4704-8718-903851c9899e$zrLKLX-4edcrChChVBWrB'}},\n",
       " 'entity': {'createdBy': 'users/edb8841f-122e-4f7d-a412-397764bc9996',\n",
       "  'createdAt': '2024-06-12T12:00:15.988Z',\n",
       "  'updatedBy': 'users/edb8841f-122e-4f7d-a412-397764bc9996',\n",
       "  'updatedAt': '2024-06-12T12:00:16.017Z',\n",
       "  'updates': [{'operation': 'create',\n",
       "    'at': '2024-06-12T12:00:16.017Z',\n",
       "    'by': 'users/edb8841f-122e-4f7d-a412-397764bc9996',\n",
       "    'comment': '',\n",
       "    'jobs': ['740799ef-d515-4704-8718-903851c9899e$zrLKLX-4edcrChChVBWrB',\n",
       "     '740799ef-d515-4704-8718-903851c9899e$zqBjfaxXehToOTcDXA-t2',\n",
       "     '740799ef-d515-4704-8718-903851c9899e$QD6cVj3WJh6_AQkrdvb6A']}],\n",
       "  'status': 'pending',\n",
       "  'runtime': {'deprecated': False,\n",
       "   'upgradable': False,\n",
       "   'name': 'plug-python3',\n",
       "   'version': '0.2.0'},\n",
       "  'deprecated': False,\n",
       "  'draft': False,\n",
       "  'plug': {'name': 'autoencoderV1',\n",
       "   'version': '0.0.1',\n",
       "   'runtime': 'plug-python3',\n",
       "   'metadata': {'tags': [{'name': 'MLAdapter', 'color': '#4153ea'}],\n",
       "    'documentation': {'description': '',\n",
       "     'states': [{'name': 'PREDICTED',\n",
       "       'description': 'The model inference succeeded.'},\n",
       "      {'name': 'FAILED', 'description': 'The model inference failed.'}],\n",
       "     'input': [{'name': 'instances',\n",
       "       'description': 'A tensor of numeric input data (KServe V1 protocol)'}],\n",
       "     'output': [{'name': 'predictions',\n",
       "       'description': 'A tensor of numeric output data (KServe V1 protocol)'},\n",
       "      {'name': 'error',\n",
       "       'description': 'Failure reasion when FAILED. the predictions will be empty.'}]}},\n",
       "   'type': 'sensor',\n",
       "   'interface': {'states': ['PREDICTED', 'FAILED'],\n",
       "    'input': [{'name': 'instances', 'dataType': 'object', 'mandatory': True}],\n",
       "    'output': [{'name': 'predictions',\n",
       "      'dataType': 'object',\n",
       "      'mandatory': True},\n",
       "     {'name': 'error', 'dataType': 'string', 'mandatory': False}]},\n",
       "   'deploy': {'limits': {'memory': '2G'}, 'requests': {'memory': '1G'}}}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = await client.ml_tool.create_plug(adapter)\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00b8fdd2-a364-4d6d-b773-de1407e7fe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 14:02:51 INFO     Waiting for autoencoderV1@0.0.1 to be ready:\n",
      "2024-06-12 14:02:51 INFO     listening on https://api-aws-dev.waylay.io/registry/v2/jobs/events?type=verify&id=740799ef-d515-4704-8718-903851c9899e$zrLKLX-4edcrChChVBWrB&children=true\n",
      "2024-06-12 14:02:51 INFO     HTTP Request: GET https://api-aws-dev.waylay.io/registry/v2/jobs/events?type=verify&id=740799ef-d515-4704-8718-903851c9899e$zrLKLX-4edcrChChVBWrB&children=true \"HTTP/1.1 200 OK\"\n",
      "2024-06-12 14:02:51 INFO     ack: Listening to events of jobs dependent on job 740799ef-d515-4704-8718-903851c9899e$zrLKLX-4edcrChChVBWrB\n",
      "2024-06-12 14:02:51 INFO     autoencoderV1@0.0.1 build: active\n",
      "2024-06-12 14:02:51 INFO     autoencoderV1@0.0.1 deploy: waiting-children\n",
      "2024-06-12 14:02:51 INFO     autoencoderV1@0.0.1 verify: waiting-children\n",
      "2024-06-12 14:03:04 INFO     keep-alive: {}\n",
      "2024-06-12 14:03:34 INFO     keep-alive: {}\n",
      "2024-06-12 14:04:04 INFO     keep-alive: {}\n",
      "2024-06-12 14:04:34 INFO     keep-alive: {}\n",
      "2024-06-12 14:05:04 INFO     keep-alive: {}\n",
      "2024-06-12 14:05:34 INFO     keep-alive: {}\n",
      "2024-06-12 14:06:04 INFO     keep-alive: {}\n",
      "2024-06-12 14:06:34 INFO     keep-alive: {}\n",
      "2024-06-12 14:07:04 INFO     keep-alive: {}\n",
      "2024-06-12 14:07:34 INFO     keep-alive: {}\n",
      "2024-06-12 14:08:04 INFO     keep-alive: {}\n",
      "2024-06-12 14:08:34 INFO     keep-alive: {}\n",
      "2024-06-12 14:08:47 INFO     autoencoderV1@0.0.1 build: completed\n",
      "{'data': {'returnvalue': {'digest': '07dbb932e89e33fbecd7c21c6aec889f844508fbc494b4924a3517d23e943619', 'log': [], 'status': 'success'}}, 'job': {'id': '740799ef-d515-4704-8718-903851c9899e$QD6cVj3WJh6_AQkrdvb6A', 'type': 'build'}, 'timestamp': '2024-06-12T12:08:47.761Z', 'function': {'name': 'autoencoderV1', 'version': '0.0.1', 'functionType': 'plugs', 'runtime': 'plug-python3', 'runtimeVersion': '0.2.0'}, '_links': {'job': {'href': 'https://api-aws-dev.waylay.io/registry/v2/jobs/build/740799ef-d515-4704-8718-903851c9899e$QD6cVj3WJh6_AQkrdvb6A'}, 'plug': {'href': 'https://api-aws-dev.waylay.io/registry/v2/plugs/autoencoderV1/versions/0.0.1'}}}\n",
      "2024-06-12 14:08:47 INFO     autoencoderV1@0.0.1 deploy: waiting\n",
      "2024-06-12 14:08:47 INFO     autoencoderV1@0.0.1 deploy: active\n",
      "2024-06-12 14:08:47 INFO     autoencoderV1@0.0.1 deploy: completed\n",
      "{'data': {'returnvalue': {'deploySpec': {'service': 'fn-459405a0456490a12ea734440535482292125551', 'image': 'eu.gcr.io/quiet-mechanic-140114/openfaas-plugs/740799ef-d515-4704-8718-903851c9899e/201cefd24b2e6e7caa7db49c849e5e655cdaeb9e:0.0.1', 'namespace': 'openfaas-fn-740799ef-d515-4704-8718-903851c9899e', 'labels': {'networking/allow-internet-egress': 'true', 'com.openfaas.scale.zero': 'true', 'com.openfaas.scale.zero-duration': '12h', 'com.openfaas.scale.min': '1', 'com.openfaas.scale.max': '10', 'com.openfaas.scale.target': '20', 'com.openfaas.scale.target-proportion': '0.90', 'com.openfaas.scale.type': 'rps', 'io.waylay.runtime.name': 'plug-python3', 'io.waylay.runtime.version': '0.2.0', 'io.waylay.tenant': '740799ef-d515-4704-8718-903851c9899e', 'io.waylay.plugs.type': 'sensor'}, 'annotations': {'io.waylay.registry.version': '2.14.0b1', 'io.waylay.function.name': 'autoencoderV1', 'io.waylay.function.version': '0.0.1', 'io.waylay.function.revision': '6c65dbe1475fbb47ebe3fbcdeb7c02149b821cbdb4894ea3ee88482d7f511e09', 'com.openfaas.health.http.initialDelay': '1s', 'com.openfaas.health.http.periodSeconds': '5s', 'com.openfaas.profile': 'fn-nodepool-profile', 'hpa.autoscaling.banzaicloud.io/minReplicas': '1', 'hpa.autoscaling.banzaicloud.io/maxReplicas': '10', 'cpu.hpa.autoscaling.banzaicloud.io/targetAverageUtilization': '80', 'memory.hpa.autoscaling.banzaicloud.io/targetAverageUtilization': '80'}, 'limits': {'memory': '2G', 'cpu': '600m'}, 'requests': {'memory': '1G', 'cpu': '25m'}}}}, 'job': {'id': '740799ef-d515-4704-8718-903851c9899e$zqBjfaxXehToOTcDXA-t2', 'type': 'deploy'}, 'timestamp': '2024-06-12T12:08:47.897Z', 'function': {'name': 'autoencoderV1', 'version': '0.0.1', 'functionType': 'plugs', 'runtime': 'plug-python3', 'runtimeVersion': '0.2.0'}, '_links': {'job': {'href': 'https://api-aws-dev.waylay.io/registry/v2/jobs/deploy/740799ef-d515-4704-8718-903851c9899e$zqBjfaxXehToOTcDXA-t2'}, 'plug': {'href': 'https://api-aws-dev.waylay.io/registry/v2/plugs/autoencoderV1/versions/0.0.1'}}}\n",
      "2024-06-12 14:08:47 INFO     autoencoderV1@0.0.1 verify: waiting\n",
      "2024-06-12 14:08:47 INFO     autoencoderV1@0.0.1 verify: active\n",
      "2024-06-12 14:09:04 INFO     keep-alive: {}\n",
      "2024-06-12 14:09:34 INFO     keep-alive: {}\n",
      "2024-06-12 14:10:04 INFO     keep-alive: {}\n",
      "2024-06-12 14:10:34 INFO     keep-alive: {}\n",
      "2024-06-12 14:10:36 INFO     close: Job with id 740799ef-d515-4704-8718-903851c9899e$zrLKLX-4edcrChChVBWrB has completed\n",
      "2024-06-12 14:10:36 INFO     HTTP Request: GET https://api-aws-dev.waylay.io/registry/v2/plugs/autoencoderV1/versions/0.0.1 \"HTTP/1.1 200 OK\"\n",
      "2024-06-12 14:10:36 INFO     function autoencoderV1@0.0.1 has status running\n"
     ]
    }
   ],
   "source": [
    "# wait until the build, deploy and verify jobs for the webscript have finished\n",
    "# NOTE: this might take quite a few minutes, as the dependencies for torch webscripts are quite big and the images not yet optimised\n",
    "ref = await client.ml_tool.wait_until_ready(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30353da7-0331-4dee-b09a-7de2957894df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 14:21:13 INFO     HTTP Request: POST https://api-aws-dev.waylay.io/rules/v1/sensors/autoencoderV1/versions/0.0.1 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07556850463151932,\n",
       " 0.12324536591768265,\n",
       " 0.035443682223558426,\n",
       " 0.015328466892242432,\n",
       " 0.12076683342456818,\n",
       " 0.04073513671755791,\n",
       " 0.09185449779033661,\n",
       " 0.09667740762233734,\n",
       " 0.01210204977542162,\n",
       " 0.23554003238677979,\n",
       " 0.10270112752914429,\n",
       " 0.08770088851451874,\n",
       " 0.16406969726085663,\n",
       " 0.03420836478471756,\n",
       " -0.009292409755289555,\n",
       " 0.002525780349969864,\n",
       " 0.08540026843547821,\n",
       " -0.10478952527046204,\n",
       " 0.01709790527820587,\n",
       " 0.13751532137393951]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the webscript invocation\n",
    "await client.ml_tool.test_plug(ref, x_data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d5c730b-9929-49ad-bce9-242f174ebb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 14:22:00 INFO     HTTP Request: DELETE https://api-aws-dev.waylay.io/registry/v2/plugs/autoencoderV1/versions/0.0.1?force=true \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Removing plug version autoencoderV1@0.0.1',\n",
       " '_links': {'event': {'href': 'https://api-aws-dev.waylay.io/registry/v2/jobs/events?type=undeploy&id=740799ef-d515-4704-8718-903851c9899e$T5YgeLbiSiHJFPSvUBAjf&children=true'},\n",
       "  'job': {'href': 'https://api-aws-dev.waylay.io/registry/v2/jobs/undeploy/740799ef-d515-4704-8718-903851c9899e$T5YgeLbiSiHJFPSvUBAjf'}},\n",
       " 'versions': ['0.0.1']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the webscript\n",
    "await client.ml_tool.remove(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fc1aca-9fc6-4ae3-926c-8a8d0f67aa08",
   "metadata": {},
   "source": [
    "#### About `ml_tool`\n",
    "The `client.ml_tool` methods are essentialy wrappers around the methods of the [registry](https://docs.waylay.io/openapi/public/redocly/registry.html#tag/plugs) service. Alternatively you can use the [`client.registry.plug`](https://github.com/waylayio/waylay-sdk-registry-py) methods directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c1634-cc6b-45d2-99dc-2c9db9f4afbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
