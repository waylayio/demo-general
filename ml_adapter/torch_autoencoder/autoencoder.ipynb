{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46523964-f381-46ff-b009-76b32cdc3b00",
   "metadata": {},
   "source": [
    "# A pytorch ML adapter demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0a0400-976b-4e31-b681-e09d61d2881d",
   "metadata": {},
   "source": [
    " 1. [Pytorch](#pytorch) model example.\n",
    " 2. Create [Adapter](#adapter).\n",
    " 3. How to create a [Webscript](#webscript).\n",
    " 4. How to create a [Plug](#plug)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd854b-9a6e-4bac-9650-5d86b3ea56d5",
   "metadata": {},
   "source": [
    "#### Preamble\n",
    "Demo of a relatively small pytorch model.\n",
    "This notebook uses the [ml_adapter_torch](../../env/ml_adapter_torch) dependencies.\n",
    "To start: \n",
    "```\n",
    "bin/jupyter_notebook env/ml_adapter_torch ml_adapter/torch_autoencoder\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a53829b8-6fde-490f-a870-c5f540f3e03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d623cc-fad6-4d2c-9e43-8bb9ccde3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sdk profile used to connect\n",
    "PROFILE='_default_'\n",
    "LOG_LEVEL='INFO'\n",
    "MODEL_NAME='autoencoderV1'\n",
    "MODEL_VERSION='1.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dfbefcf-84f8-40d2-9393-ed3988630147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup INFO logging to see http requests made.\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=LOG_LEVEL,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e989be6-88be-40c9-b80a-e77d5f152eb8",
   "metadata": {},
   "source": [
    "## 1. Pytorch example <a id=\"pytorch\"></a>\n",
    "\n",
    "A simple auto-encoder pytorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fbe4087-26be-4945-8f1a-96fab99d3669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b779cd6-2c39-4efa-95ef-6a8389ec7820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "\n",
       "<span class=\"k\">class</span> <span class=\"nc\">AutoEncoder</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n",
       "    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
       "        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n",
       "<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;</span>\n",
       "<span class=\"sd\">        Create a simple AutoEncoder</span>\n",
       "<span class=\"sd\">        &quot;&quot;&quot;</span>\n",
       "        <span class=\"c1\"># Use an AutoEncoder and try to reconstruct both signals (the 20 samples back)</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">encoder</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">),</span>\n",
       "        <span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">decoder</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">),</span>\n",
       "        <span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n",
       "        <span class=\"n\">encoded</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">encoder</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">decoded</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">decoder</span><span class=\"p\">(</span><span class=\"n\">encoded</span><span class=\"p\">)</span>\n",
       "        <span class=\"k\">return</span> <span class=\"n\">decoded</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\n",
       "\\PY{k}{class} \\PY{n+nc}{AutoEncoder}\\PY{p}{(}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Module}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{k}{def} \\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n+nb}{super}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{+w}{        }\\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "\\PY{l+s+sd}{        Create a simple AutoEncoder}\n",
       "\\PY{l+s+sd}{        \\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "        \\PY{c+c1}{\\PYZsh{} Use an AutoEncoder and try to reconstruct both signals (the 20 samples back)}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{encoder} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Sequential}\\PY{p}{(}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{20}\\PY{p}{,} \\PY{l+m+mi}{10}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{ReLU}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{10}\\PY{p}{,} \\PY{l+m+mi}{5}\\PY{p}{)}\\PY{p}{,}\n",
       "        \\PY{p}{)}\n",
       "\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{decoder} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Sequential}\\PY{p}{(}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{5}\\PY{p}{,} \\PY{l+m+mi}{10}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{ReLU}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{10}\\PY{p}{,} \\PY{l+m+mi}{20}\\PY{p}{)}\\PY{p}{,}\n",
       "        \\PY{p}{)}\n",
       "\n",
       "    \\PY{k}{def} \\PY{n+nf}{forward}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{x}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n}{encoded} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{encoder}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n",
       "        \\PY{n}{decoded} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{decoder}\\PY{p}{(}\\PY{n}{encoded}\\PY{p}{)}\n",
       "        \\PY{k}{return} \\PY{n}{decoded}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "import torch\n",
       "\n",
       "class AutoEncoder(torch.nn.Module):\n",
       "    def __init__(self):\n",
       "        super().__init__()\n",
       "        \"\"\"\n",
       "        Create a simple AutoEncoder\n",
       "        \"\"\"\n",
       "        # Use an AutoEncoder and try to reconstruct both signals (the 20 samples back)\n",
       "        self.encoder = torch.nn.Sequential(\n",
       "            torch.nn.Linear(20, 10),\n",
       "            torch.nn.ReLU(),\n",
       "            torch.nn.Linear(10, 5),\n",
       "        )\n",
       "\n",
       "        self.decoder = torch.nn.Sequential(\n",
       "            torch.nn.Linear(5, 10),\n",
       "            torch.nn.ReLU(),\n",
       "            torch.nn.Linear(10, 20),\n",
       "        )\n",
       "\n",
       "    def forward(self, x):\n",
       "        encoded = self.encoder(x)\n",
       "        decoded = self.decoder(encoded)\n",
       "        return decoded"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we saved our model class in a `autoencoder.py` file\n",
    "from IPython.display import Code, Markdown\n",
    "display(Code(filename='autoencoder.py'))\n",
    "from autoencoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5409ee2-ffb0-450b-91a6-9be5b857c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some pretrained weights\n",
    "weights_path = 'AutoEncoderWeights.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22a7a6e6-ece7-49d3-8bba-a128ebc621c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=5, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = AutoEncoder()\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f74982-aea0-4d08-8ef7-71c7fec68a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1732,  0.0324,  0.0194,  0.8020, -1.3497,  0.8811, -1.9011, -0.4583,\n",
       "        -1.8547,  0.5366,  1.2801, -0.4675, -1.5484,  0.8876,  0.1250, -0.4512,\n",
       "         2.4818, -1.4263,  1.6530,  0.1223])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = torch.randn(20, dtype=torch.float32)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cef5213-7a07-4360-80b1-6bd352a188a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0840, -0.1075, -0.1180, -0.0856, -0.1301, -0.1541, -0.1372, -0.1259,\n",
       "        -0.1530, -0.0339, -0.1280, -0.0944, -0.0869, -0.1309, -0.1258, -0.1441,\n",
       "        -0.0714, -0.1667, -0.1455, -0.1114], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(x_data)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc56c20c-e56c-476b-ba70-27e40a479b1f",
   "metadata": {},
   "source": [
    "## 2. The adapter <a id=\"adapter\"></a>\n",
    "The `V1TorchAdapter` from the `ml_adapter.torch` module wraps our model in a script that can be used as a waylay webscript or plug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7023e2eb-4ebb-4f5e-a436-5e11b07222e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a local archive location\n",
    "ARCHIVE_LOC = 'autoencoder-pytorch'\n",
    "# make sure its empty\n",
    "!rm -fr autoencoder-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc2c179-9eb8-4406-ba6a-ef38f6786336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_adapter.torch import V1TorchAdapter\n",
    "# create an ML adapter to wrap our model\n",
    "# by using a `weights.pt` postfix we are storing only the weights when serializing the model\n",
    "MODEL_PATH='model-weights.pt'\n",
    "adapter = V1TorchAdapter(model=model, model_path='model-weights.pt', location=ARCHIVE_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47bc96ed-9f3c-4958-a718-4eb7b18735d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [[-0.0839521661400795,\n",
       "   -0.10749989002943039,\n",
       "   -0.11796973645687103,\n",
       "   -0.08562793582677841,\n",
       "   -0.13007289171218872,\n",
       "   -0.1541411578655243,\n",
       "   -0.1371534764766693,\n",
       "   -0.12586581707000732,\n",
       "   -0.15304678678512573,\n",
       "   -0.03389810025691986,\n",
       "   -0.12804251909255981,\n",
       "   -0.0943613275885582,\n",
       "   -0.08690498769283295,\n",
       "   -0.13089188933372498,\n",
       "   -0.12576353549957275,\n",
       "   -0.14411213994026184,\n",
       "   -0.07141607999801636,\n",
       "   -0.16669408977031708,\n",
       "   -0.1455150693655014,\n",
       "   -0.11138710379600525]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the adapter exposes your model with a REST-compatible interface\n",
    "result = await adapter.call({\"instances\": [ x_data.tolist() ]}) \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7c77d97-61a2-488c-8e07-8c894b7a8f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autoencoder.py <ml_adapter.base.assets.python.PythonScriptAsset>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because we store only weights, the adapter archive needs to now about autoencode model class:\n",
    "await adapter.add_script('autoencoder.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4807ddc-7c5e-4e01-859c-202062ab690a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openapi.json',\n",
       " 'webscript.json',\n",
       " 'requirements.txt',\n",
       " 'main.py',\n",
       " 'model-weights.pt',\n",
       " 'autoencoder.py']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await adapter.save()\n",
    "# have a look at ARCHIVE_LOC to see the stored assets\n",
    "list(a.path for a in adapter.assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9872aa-6cab-43ea-9fd4-87f1a43390fc",
   "metadata": {},
   "source": [
    "## 3. Creating the webscript <a id=\"webscript\"></a>\n",
    "Tell the adapter to configure itself as a webscript: this generates a number of _assets_ that will be uploaded and define the webscript behaviour\n",
    "* a `webscript.json` _manifest_ file that defines the name, version, _runtime_, deploy settings, metadata ...\n",
    "* a `requirements.txt` package dependencies file\n",
    "* a `main.py` webscript script\n",
    "* additional scripts we added above, like the `autoencoder.py` and the `model-weights.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f68556cd-921d-495d-93d1-a94c7897d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## configure any 'memory' or 'cpu' deploy settings \n",
    "deploy_overrides = {'limits' : { 'memory': '2G' }, 'requests' : { 'memory' : '1G' }}\n",
    "## configure the webscript to use our model\n",
    "adapter = adapter.as_webscript({\n",
    "    'name': MODEL_NAME, \n",
    "    'description':'pytorch autoencoder for caats', \n",
    "    'deploy' : deploy_overrides                                                                                          \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e75891d-87fc-4b58-925f-e779595a3b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openapi.json',\n",
       " 'webscript.json',\n",
       " 'requirements.txt',\n",
       " 'main.py',\n",
       " 'model-weights.pt',\n",
       " 'autoencoder.py']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await adapter.save()\n",
    "list(a.path for a in adapter.assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c2ce077-f293-4c2f-9c6f-7784ddb3fdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># ml_adapter.torch.adapter.V1TorchAdapter model adapter</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">os</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">starlette.requests</span> <span class=\"kn\">import</span> <span class=\"n\">Request</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">starlette.responses</span> <span class=\"kn\">import</span> <span class=\"n\">JSONResponse</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">starlette.exceptions</span> <span class=\"kn\">import</span> <span class=\"n\">HTTPException</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">ml_adapter.torch.adapter</span> <span class=\"kn\">import</span> <span class=\"n\">V1TorchAdapter</span>\n",
       "\n",
       "<span class=\"n\">MODEL_PATH</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">&#39;MODEL_PATH&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;model-weights.pt&#39;</span><span class=\"p\">)</span>\n",
       "<span class=\"n\">MODEL_CLASS</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">&#39;MODEL_CLASS&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;autoencoder.AutoEncoder&#39;</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Initialize the model adapter.</span>\n",
       "<span class=\"c1\"># Provide a `model` argument if you want to create/load the model yourself.</span>\n",
       "<span class=\"n\">adapter</span> <span class=\"o\">=</span> <span class=\"n\">V1TorchAdapter</span><span class=\"p\">(</span>\n",
       "    <span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"n\">MODEL_PATH</span><span class=\"p\">,</span> <span class=\"n\">model_class</span><span class=\"o\">=</span><span class=\"n\">MODEL_CLASS</span>\n",
       "<span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Webscript handler</span>\n",
       "<span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"p\">:</span> <span class=\"n\">Request</span><span class=\"p\">):</span>\n",
       "    <span class=\"k\">if</span> <span class=\"n\">request</span><span class=\"o\">.</span><span class=\"n\">method</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;GET&#39;</span><span class=\"p\">:</span>\n",
       "        <span class=\"k\">return</span> <span class=\"n\">JSONResponse</span><span class=\"p\">(</span><span class=\"n\">adapter</span><span class=\"o\">.</span><span class=\"n\">openapi</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">if</span> <span class=\"n\">request</span><span class=\"o\">.</span><span class=\"n\">method</span> <span class=\"o\">!=</span> <span class=\"s1\">&#39;POST&#39;</span><span class=\"p\">:</span>\n",
       "        <span class=\"k\">raise</span> <span class=\"n\">HTTPException</span><span class=\"p\">(</span>\n",
       "            <span class=\"n\">status_code</span><span class=\"o\">=</span><span class=\"mi\">405</span><span class=\"p\">,</span>\n",
       "            <span class=\"n\">detail</span><span class=\"o\">=</span><span class=\"s1\">&#39;This webscript only accepts `POST` calls.&#39;</span><span class=\"p\">,</span>\n",
       "        <span class=\"p\">)</span>\n",
       "    <span class=\"c1\"># use request body as input</span>\n",
       "    <span class=\"n\">request_json</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">request</span><span class=\"o\">.</span><span class=\"n\">json</span><span class=\"p\">()</span>\n",
       "    <span class=\"c1\"># call the model adapter using the V1</span>\n",
       "    <span class=\"n\">response_json</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">adapter</span><span class=\"o\">.</span><span class=\"n\">call</span><span class=\"p\">(</span><span class=\"n\">request_json</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">JSONResponse</span><span class=\"p\">(</span><span class=\"n\">response_json</span><span class=\"p\">)</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{} ml\\PYZus{}adapter.torch.adapter.V1TorchAdapter model adapter}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{os}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{starlette}\\PY{n+nn}{.}\\PY{n+nn}{requests} \\PY{k+kn}{import} \\PY{n}{Request}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{starlette}\\PY{n+nn}{.}\\PY{n+nn}{responses} \\PY{k+kn}{import} \\PY{n}{JSONResponse}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{starlette}\\PY{n+nn}{.}\\PY{n+nn}{exceptions} \\PY{k+kn}{import} \\PY{n}{HTTPException}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{ml\\PYZus{}adapter}\\PY{n+nn}{.}\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{adapter} \\PY{k+kn}{import} \\PY{n}{V1TorchAdapter}\n",
       "\n",
       "\\PY{n}{MODEL\\PYZus{}PATH} \\PY{o}{=} \\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{o}{.}\\PY{n}{get}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{MODEL\\PYZus{}PATH}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{model\\PYZhy{}weights.pt}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "\\PY{n}{MODEL\\PYZus{}CLASS} \\PY{o}{=} \\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{o}{.}\\PY{n}{get}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{MODEL\\PYZus{}CLASS}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{autoencoder.AutoEncoder}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Initialize the model adapter.}\n",
       "\\PY{c+c1}{\\PYZsh{} Provide a `model` argument if you want to create/load the model yourself.}\n",
       "\\PY{n}{adapter} \\PY{o}{=} \\PY{n}{V1TorchAdapter}\\PY{p}{(}\n",
       "    \\PY{n}{model\\PYZus{}path}\\PY{o}{=}\\PY{n}{MODEL\\PYZus{}PATH}\\PY{p}{,} \\PY{n}{model\\PYZus{}class}\\PY{o}{=}\\PY{n}{MODEL\\PYZus{}CLASS}\n",
       "\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Webscript handler}\n",
       "\\PY{k}{async} \\PY{k}{def} \\PY{n+nf}{execute}\\PY{p}{(}\\PY{n}{request}\\PY{p}{:} \\PY{n}{Request}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{k}{if} \\PY{n}{request}\\PY{o}{.}\\PY{n}{method} \\PY{o}{==} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{GET}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:}\n",
       "        \\PY{k}{return} \\PY{n}{JSONResponse}\\PY{p}{(}\\PY{n}{adapter}\\PY{o}{.}\\PY{n}{openapi}\\PY{p}{)}\n",
       "    \\PY{k}{if} \\PY{n}{request}\\PY{o}{.}\\PY{n}{method} \\PY{o}{!=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{POST}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:}\n",
       "        \\PY{k}{raise} \\PY{n}{HTTPException}\\PY{p}{(}\n",
       "            \\PY{n}{status\\PYZus{}code}\\PY{o}{=}\\PY{l+m+mi}{405}\\PY{p}{,}\n",
       "            \\PY{n}{detail}\\PY{o}{=}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{This webscript only accepts `POST` calls.}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,}\n",
       "        \\PY{p}{)}\n",
       "    \\PY{c+c1}{\\PYZsh{} use request body as input}\n",
       "    \\PY{n}{request\\PYZus{}json} \\PY{o}{=} \\PY{k}{await} \\PY{n}{request}\\PY{o}{.}\\PY{n}{json}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{c+c1}{\\PYZsh{} call the model adapter using the V1}\n",
       "    \\PY{n}{response\\PYZus{}json} \\PY{o}{=} \\PY{k}{await} \\PY{n}{adapter}\\PY{o}{.}\\PY{n}{call}\\PY{p}{(}\\PY{n}{request\\PYZus{}json}\\PY{p}{)}\n",
       "    \\PY{k}{return} \\PY{n}{JSONResponse}\\PY{p}{(}\\PY{n}{response\\PYZus{}json}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "\n",
       "# ml_adapter.torch.adapter.V1TorchAdapter model adapter\n",
       "import os\n",
       "from starlette.requests import Request\n",
       "from starlette.responses import JSONResponse\n",
       "from starlette.exceptions import HTTPException\n",
       "from ml_adapter.torch.adapter import V1TorchAdapter\n",
       "\n",
       "MODEL_PATH = os.environ.get('MODEL_PATH', 'model-weights.pt')\n",
       "MODEL_CLASS = os.environ.get('MODEL_CLASS', 'autoencoder.AutoEncoder')\n",
       "\n",
       "# Initialize the model adapter.\n",
       "# Provide a `model` argument if you want to create/load the model yourself.\n",
       "adapter = V1TorchAdapter(\n",
       "    model_path=MODEL_PATH, model_class=MODEL_CLASS\n",
       ")\n",
       "\n",
       "# Webscript handler\n",
       "async def execute(request: Request):\n",
       "    if request.method == 'GET':\n",
       "        return JSONResponse(adapter.openapi)\n",
       "    if request.method != 'POST':\n",
       "        raise HTTPException(\n",
       "            status_code=405,\n",
       "            detail='This webscript only accepts `POST` calls.',\n",
       "        )\n",
       "    # use request body as input\n",
       "    request_json = await request.json()\n",
       "    # call the model adapter using the V1\n",
       "    response_json = await adapter.call(request_json)\n",
       "    return JSONResponse(response_json)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets have a look at the generated python webscript:\n",
    "display(Code(filename=f'{ARCHIVE_LOC}/main.py'))\n",
    "\n",
    "# You could adapt this script to have specific error handling or handling of request/response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d65e204-c421-4026-a337-330dcbeca22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once stored, the adapter can be restored later with\n",
    "adapter = await V1TorchAdapter(model_path='model-weights.pt', model_class=AutoEncoder, location=ARCHIVE_LOC).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9d9e482-cf73-4631-b94a-a732b158701f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[openapi.json <ml_adapter.base.assets.openapi.OpenApiAsset>,\n",
       " webscript.json <ml_adapter.base.assets.manifest.WebscriptManifestAsset>,\n",
       " requirements.txt <ml_adapter.base.assets.python.PythonRequirementsAsset>,\n",
       " main.py <ml_adapter.base.assets.python.PythonScriptAsset>,\n",
       " model-weights.pt <ml_adapter.torch.adapter.TorchModelWeightsAsset>,\n",
       " autoencoder.py <ml_adapter.base.assets.python.PythonScriptAsset>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(adapter.assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e0183-cf8c-4225-965e-89b75b3c781d",
   "metadata": {},
   "source": [
    "### Uploading the webscript using the SDK\n",
    "To upload these assets and create a webscript, we need to call the [create webcript](https://docs.waylay.io/openapi/public/redocly/registry.html#tag/Webscripts/operation/create_webscripts) REST api. \n",
    "\n",
    "The code belows uses the `ml_tool` plugin to handle this.\n",
    "Alternatively you could call `await adapter.save_archive()`\n",
    "which creates an `autoencoder-pytorch.tar.gz` archive that you can upload as webscript with `curl` or using the waylay console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbb7b49c-cc60-4e8a-afae-a2908a6caf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from waylay.sdk import WaylayClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecca79ac-ad09-43c4-876a-7a5e9fd7b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the SDK client\n",
    "client = WaylayClient.from_profile('staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70434f0b-6bdc-4ca8-8214-0dd40ba5bfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 11:47:25 INFO     HTTP Request: POST https://api-aws-dev.waylay.io/accounts/v1/tokens?grant_type=client_credentials \"HTTP/1.1 200 OK\"\n",
      "2024-06-11 11:47:26 INFO     HTTP Request: POST https://api-aws-dev.waylay.io/registry/v2/webscripts/?draft=false&comment=&async=true \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Building and deploying webscript autoencoder-pytorch-v1@0.0.1',\n",
       " '_links': {'event': {'href': 'https://api-aws-dev.waylay.io/registry/v2/jobs/events?type=verify&id=740799ef-d515-4704-8718-903851c9899e$BO6yELncbGqLydPhWzvFE&children=true'},\n",
       "  'job': {'href': 'https://api-aws-dev.waylay.io/registry/v2/jobs/verify/740799ef-d515-4704-8718-903851c9899e$BO6yELncbGqLydPhWzvFE'}},\n",
       " 'entity': {'createdBy': 'users/edb8841f-122e-4f7d-a412-397764bc9996',\n",
       "  'createdAt': '2024-06-11T09:47:26.176Z',\n",
       "  'updatedBy': 'users/edb8841f-122e-4f7d-a412-397764bc9996',\n",
       "  'updatedAt': '2024-06-11T09:47:26.199Z',\n",
       "  'updates': [{'operation': 'create',\n",
       "    'at': '2024-06-11T09:47:26.199Z',\n",
       "    'by': 'users/edb8841f-122e-4f7d-a412-397764bc9996',\n",
       "    'comment': '',\n",
       "    'jobs': ['740799ef-d515-4704-8718-903851c9899e$BO6yELncbGqLydPhWzvFE',\n",
       "     '740799ef-d515-4704-8718-903851c9899e$1zoyGfeHuap8L6MdI9hlV',\n",
       "     '740799ef-d515-4704-8718-903851c9899e$VQdbRM4WJxFgmjvIp4TrC']}],\n",
       "  'status': 'pending',\n",
       "  'runtime': {'deprecated': False,\n",
       "   'upgradable': False,\n",
       "   'name': 'web-python3',\n",
       "   'version': '0.2.0'},\n",
       "  'deprecated': False,\n",
       "  'draft': False,\n",
       "  'webscript': {'name': 'autoencoder-pytorch-v1',\n",
       "   'version': '0.0.1',\n",
       "   'runtime': 'web-python3',\n",
       "   'metadata': {},\n",
       "   'private': True,\n",
       "   'allowHmac': True,\n",
       "   'deploy': {'limits': {'memory': '2G'}, 'requests': {'memory': '1G'}}},\n",
       "  'secret': 'z3ULTbGNvSPEI4v2wV6uT2RAv7i7LpoGSGKkjQ1ESzI='}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = await client.ml_tool.create_webscript(adapter)\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00b8fdd2-a364-4d6d-b773-de1407e7fe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 11:47:36 INFO     Waiting for autoencoder-pytorch-v1@0.0.1 to be ready:\n",
      "2024-06-11 11:47:36 INFO     listening on https://api-aws-dev.waylay.io/registry/v2/jobs/events?type=verify&id=740799ef-d515-4704-8718-903851c9899e$BO6yELncbGqLydPhWzvFE&children=true\n",
      "2024-06-11 11:47:37 INFO     HTTP Request: GET https://api-aws-dev.waylay.io/registry/v2/jobs/events?type=verify&id=740799ef-d515-4704-8718-903851c9899e$BO6yELncbGqLydPhWzvFE&children=true \"HTTP/1.1 200 OK\"\n",
      "2024-06-11 11:47:37 INFO     autoencoder-pytorch-v1@0.0.1 deploy: waiting-children\n",
      "2024-06-11 11:47:52 INFO     keep-alive: {}\n",
      "2024-06-11 11:48:22 INFO     keep-alive: {}\n",
      "2024-06-11 11:48:51 INFO     keep-alive: {}\n",
      "2024-06-11 11:49:22 INFO     keep-alive: {}\n",
      "2024-06-11 11:49:52 INFO     keep-alive: {}\n",
      "2024-06-11 11:50:22 INFO     keep-alive: {}\n",
      "2024-06-11 11:50:52 INFO     keep-alive: {}\n",
      "2024-06-11 11:51:21 INFO     keep-alive: {}\n",
      "2024-06-11 11:51:52 INFO     keep-alive: {}\n",
      "2024-06-11 11:52:22 INFO     keep-alive: {}\n",
      "2024-06-11 11:52:51 INFO     keep-alive: {}\n",
      "2024-06-11 11:53:22 INFO     keep-alive: {}\n",
      "2024-06-11 11:53:51 INFO     keep-alive: {}\n",
      "2024-06-11 11:54:21 INFO     keep-alive: {}\n",
      "2024-06-11 11:54:52 INFO     keep-alive: {}\n",
      "2024-06-11 11:55:22 INFO     keep-alive: {}\n",
      "2024-06-11 11:55:52 INFO     keep-alive: {}\n",
      "2024-06-11 11:56:22 INFO     keep-alive: {}\n",
      "2024-06-11 11:56:52 INFO     keep-alive: {}\n",
      "2024-06-11 11:57:11 INFO     autoencoder-pytorch-v1@0.0.1 build: completed\n",
      "{'data': {'returnvalue': {'digest': '889e33cb5e35197d463276fd7688a024eb9422bedf3923d6b17dbc6f5f8dc73d', 'log': [], 'status': 'success'}}, 'job': {'id': '740799ef-d515-4704-8718-903851c9899e$VQdbRM4WJxFgmjvIp4TrC', 'type': 'build'}, 'timestamp': '2024-06-11T09:57:11.974Z', 'function': {'name': 'autoencoder-pytorch-v1', 'version': '0.0.1', 'functionType': 'webscripts', 'runtime': 'web-python3', 'runtimeVersion': '0.2.0'}, '_links': {'job': {'href': 'https://api-aws-dev.waylay.io/registry/v2/jobs/build/740799ef-d515-4704-8718-903851c9899e$VQdbRM4WJxFgmjvIp4TrC'}, 'webscript': {'href': 'https://api-aws-dev.waylay.io/registry/v2/webscripts/autoencoder-pytorch-v1/versions/0.0.1'}}}\n",
      "2024-06-11 11:57:11 INFO     autoencoder-pytorch-v1@0.0.1 deploy: waiting\n",
      "2024-06-11 11:57:11 INFO     autoencoder-pytorch-v1@0.0.1 deploy: active\n",
      "2024-06-11 11:57:12 INFO     autoencoder-pytorch-v1@0.0.1 deploy: completed\n",
      "{'data': {'returnvalue': {'deploySpec': {'service': 'web-2b5c3d082690765bb9dd0b60cb85a4a6cf11ce79', 'image': 'eu.gcr.io/quiet-mechanic-140114/openfaas-plugs/740799ef-d515-4704-8718-903851c9899e/1cf5a5c839f5331986ac3df53ae3fd100cfcb48b:0.0.1', 'namespace': 'openfaas-fn-740799ef-d515-4704-8718-903851c9899e', 'labels': {'networking/allow-internet-egress': 'true', 'com.openfaas.scale.zero': 'true', 'com.openfaas.scale.zero-duration': '12h', 'com.openfaas.scale.min': '1', 'com.openfaas.scale.max': '10', 'com.openfaas.scale.target': '20', 'com.openfaas.scale.target-proportion': '0.90', 'com.openfaas.scale.type': 'rps', 'io.waylay.runtime.name': 'web-python3', 'io.waylay.runtime.version': '0.2.0', 'io.waylay.tenant': '740799ef-d515-4704-8718-903851c9899e'}, 'annotations': {'io.waylay.registry.version': '2.14.0b1', 'io.waylay.function.name': 'autoencoder-pytorch-v1', 'io.waylay.function.version': '0.0.1', 'io.waylay.function.revision': '4aa7c7e59fd7d1176c613a8d163f9cc532fa45a59b96b85c1b8b8cc745f3a199', 'com.openfaas.health.http.initialDelay': '1s', 'com.openfaas.health.http.periodSeconds': '5s', 'com.openfaas.profile': 'fn-nodepool-profile', 'hpa.autoscaling.banzaicloud.io/minReplicas': '1', 'hpa.autoscaling.banzaicloud.io/maxReplicas': '10', 'cpu.hpa.autoscaling.banzaicloud.io/targetAverageUtilization': '80', 'memory.hpa.autoscaling.banzaicloud.io/targetAverageUtilization': '80'}, 'limits': {'memory': '2G', 'cpu': '150m'}, 'requests': {'memory': '1G', 'cpu': '25m'}}}}, 'job': {'id': '740799ef-d515-4704-8718-903851c9899e$1zoyGfeHuap8L6MdI9hlV', 'type': 'deploy'}, 'timestamp': '2024-06-11T09:57:12.487Z', 'function': {'name': 'autoencoder-pytorch-v1', 'version': '0.0.1', 'functionType': 'webscripts', 'runtime': 'web-python3', 'runtimeVersion': '0.2.0'}, '_links': {'job': {'href': 'https://api-aws-dev.waylay.io/registry/v2/jobs/deploy/740799ef-d515-4704-8718-903851c9899e$1zoyGfeHuap8L6MdI9hlV'}, 'webscript': {'href': 'https://api-aws-dev.waylay.io/registry/v2/webscripts/autoencoder-pytorch-v1/versions/0.0.1'}}}\n",
      "2024-06-11 11:57:12 INFO     autoencoder-pytorch-v1@0.0.1 verify: waiting\n",
      "2024-06-11 11:57:12 INFO     autoencoder-pytorch-v1@0.0.1 verify: active\n",
      "2024-06-11 11:57:22 INFO     keep-alive: {}\n",
      "2024-06-11 11:57:52 INFO     keep-alive: {}\n",
      "2024-06-11 11:58:22 INFO     keep-alive: {}\n",
      "2024-06-11 11:58:51 INFO     keep-alive: {}\n",
      "2024-06-11 11:59:22 INFO     keep-alive: {}\n",
      "2024-06-11 11:59:51 INFO     keep-alive: {}\n",
      "2024-06-11 12:00:22 INFO     keep-alive: {}\n",
      "2024-06-11 12:00:52 INFO     keep-alive: {}\n",
      "2024-06-11 12:01:07 INFO     close: Job with id 740799ef-d515-4704-8718-903851c9899e$BO6yELncbGqLydPhWzvFE has completed\n",
      "2024-06-11 12:01:07 INFO     HTTP Request: GET https://api-aws-dev.waylay.io/registry/v2/webscripts/autoencoder-pytorch-v1/versions/0.0.1 \"HTTP/1.1 200 OK\"\n",
      "2024-06-11 12:01:07 INFO     function autoencoder-pytorch-v1@0.0.1 has status running\n"
     ]
    }
   ],
   "source": [
    "# wait until the build, deploy and verify jobs for the webscript have finished\n",
    "# NOTE: this might take quite a few minutes, as the dependencies for torch webscripts are quite big and the images not yet optimised\n",
    "ref = await client.ml_tool.wait_until_ready(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30353da7-0331-4dee-b09a-7de2957894df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 12:03:12 INFO     HTTP Request: POST https://api-aws-dev.waylay.io/functions/v1/740799ef-d515-4704-8718-903851c9899e/autoencoder-pytorch-v1 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.0839521661400795,\n",
       " -0.10749989002943039,\n",
       " -0.11796973645687103,\n",
       " -0.08562793582677841,\n",
       " -0.13007290661334991,\n",
       " -0.1541411578655243,\n",
       " -0.1371534764766693,\n",
       " -0.12586581707000732,\n",
       " -0.15304680168628693,\n",
       " -0.033898092806339264,\n",
       " -0.12804250419139862,\n",
       " -0.0943613275885582,\n",
       " -0.08690498769283295,\n",
       " -0.13089188933372498,\n",
       " -0.12576353549957275,\n",
       " -0.14411213994026184,\n",
       " -0.07141607999801636,\n",
       " -0.16669408977031708,\n",
       " -0.1455150693655014,\n",
       " -0.11138710379600525]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the webscript invocation\n",
    "await client.ml_tool.test_webscript(ref, x_data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d5c730b-9929-49ad-bce9-242f174ebb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 12:57:21 INFO     HTTP Request: DELETE https://api-aws-dev.waylay.io/registry/v2/webscripts/autoencoder-pytorch-v1/versions/0.0.1?force=true \"HTTP/1.1 202 Accepted\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Deleting webscript autoencoder-pytorch-v1@0.0.1',\n",
       " '_links': {'event': {'href': 'https://api-aws-dev.waylay.io/registry/v2/jobs/events?type=undeploy&id=740799ef-d515-4704-8718-903851c9899e$EPOirtvaoVc1LYUm-Lj5U&children=true'},\n",
       "  'job': {'href': 'https://api-aws-dev.waylay.io/registry/v2/jobs/undeploy/740799ef-d515-4704-8718-903851c9899e$EPOirtvaoVc1LYUm-Lj5U'}},\n",
       " 'versions': ['0.0.1']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the webscript\n",
    "await client.ml_tool.remove(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95dcfc-fe9a-4370-9e96-443bc22e8ce9",
   "metadata": {},
   "source": [
    "## 4. Creating the plug <a id=\"plug\"></a>\n",
    "Tell the adapter to configure itself as a plug: this generates a number of _assets_ that will be uploaded and define the plug behaviour\n",
    "* a `plug.json` _manifest_ file that defines the name, version, _runtime_, deploy settings, metadata ...\n",
    "* a `requirements.txt` package dependencies file\n",
    "* a `main.py` webscript script\n",
    "* additional scripts we added above, like the `autoencoder.py` and the `model-weights.pt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eba12a-193e-42e6-84ae-641fe71ae465",
   "metadata": {},
   "source": [
    "#### Intiliaze adapter for plug deployment (see how to create [adapter](#adapter))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3104e1c-d950-4f1f-b407-0b171bd5cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a local archive location\n",
    "ARCHIVE_LOC = 'autoencoder-pytorch'\n",
    "# make sure its empty\n",
    "!rm -fr autoencoder-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef413e89-743e-44ad-a01f-d93813dfeb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_adapter.torch import V1TorchAdapter\n",
    "# create an ML adapter to wrap our model\n",
    "# by using a `weights.pt` postfix we are storing only the weights when serializing the model\n",
    "MODEL_PATH='model-weights.pt'\n",
    "adapter = V1TorchAdapter(model=model, model_path='model-weights.pt', location=ARCHIVE_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b504183-7e1a-4cf9-87f5-ce1615163264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the adapter exposes your model with a REST-compatible interface\n",
    "result = await adapter.call({\"instances\": [ x_data.tolist() ]}) \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097fdc6a-ae00-49ff-ad1d-49d1aeaf2fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## configure any 'memory' or 'cpu' deploy settings \n",
    "deploy_overrides = {'limits' : { 'memory': '2G' }, 'requests' : { 'memory' : '1G' }}\n",
    "## configure the webscript to use our model\n",
    "adapter = adapter.as_plug({\n",
    "    'name': MODEL_NAME, \n",
    "    'description':'pytorch autoencoder for caats', \n",
    "    'deploy' : deploy_overrides                                                                                          \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4181dc8f-07e3-4c94-b8e4-97bae06dcbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "await adapter.save()\n",
    "list(a.path for a in adapter.assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52918956-9691-4b24-946d-873bd8af5b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at the generated python plug:\n",
    "display(Code(filename=f'{ARCHIVE_LOC}/main.py'))\n",
    "\n",
    "# You could adapt this script to have specific error handling or handling of request/response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d602439-bd03-4f37-a4d0-0c8f2b2d1e29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(a.path for a in adapter.assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56979c72-5a25-4a23-ad00-14c1812cb2fd",
   "metadata": {},
   "source": [
    "### Uploading the plug using the SDK\n",
    "To upload these assets and create a plug, we need to call the [create plug](https://docs.waylay.io/openapi/public/redocly/registry.html#tag/Plugs/operation/create_plugs) REST api. \n",
    "\n",
    "The code belows uses the `ml_tool` plugin to handle this.\n",
    "Alternatively you could call `await adapter.save_archive()`\n",
    "which creates an `autoencoder-pytorch.tar.gz` archive that you can upload as a plug with `curl` or using the waylay console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7342717b-77f3-4fa0-8d29-99090b1b64df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from waylay.sdk import WaylayClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f1c86-d6e6-4d01-a43a-19533f35f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the SDK client\n",
    "client = WaylayClient.from_profile('staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0010c395-aca0-4f10-80aa-ba64a6daf660",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = await client.ml_tool.create_plug(adapter)\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e7c0d0-d3ac-482c-b1c9-809d2f358db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait until the build, deploy and verify jobs for the webscript have finished\n",
    "# NOTE: this might take quite a few minutes, as the dependencies for torch webscripts are quite big and the images not yet optimised\n",
    "ref = await client.ml_tool.wait_until_ready(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7328f-79db-4b4c-8e61-cb332a1adac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the plug invocation\n",
    "await client.ml_tool.test_plug(ref, x_data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a03afe-e35c-441d-bdb1-f56c13a33622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the plug\n",
    "await client.ml_tool.remove(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fc1aca-9fc6-4ae3-926c-8a8d0f67aa08",
   "metadata": {},
   "source": [
    "#### About `ml_tool`\n",
    "The `client.ml_tool` methods are essentialy wrappers around the methods of the [registry](https://docs.waylay.io/openapi/public/redocly/registry.html#tag/Webscripts) service. Alternatively you can use the [`client.registry.webscript`](https://github.com/waylayio/waylay-sdk-registry-py) methods directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d7de2-4fcd-49cc-908d-343599dcb241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
